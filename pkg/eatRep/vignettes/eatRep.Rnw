\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{eatRep: a package to analyze multiple imputed data in complex survey designs}
%%\VignetteDepends{eatRep}
%%
\newcommand{\trans}{\ensuremath{^\mathsf{T}}}
\newcommand{\invtrans}{\ensuremath{^\mathsf{-T}}}
\title{eatRep: a package to analyze multiple imputed data in complex survey designs}
\author{Sebastian Weirich\\Humboldt University Berlin, Germany}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=true,keep.source=TRUE}
\SweaveOpts{include=FALSE}
\setkeys{Gin}{width=\textwidth}
\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\package}[1]{\textsf{\small{#1}}}
\maketitle
\begin{abstract}
  Estimation of simple descriptive statistics becomes cumbersome, if the sample cannot be considered to be a (completely) random draw
  from the population for which descriptives should be interpreted. This occurs in weighted samples or clustered samples. The same is true
  if the variables of interest stem from a multiple imputation process and occur, for example, as plausible values.
  This tutorial describes some basic analyses to compute descriptives in complex survey designs using the R package \code{eatRep}, which was
  designed mainly to supply replications methods in R. To date, only the Jackknife-2 (JK2) method is supported. Some functions overlap with 
  methods provided in the computer software WesVar \citep{Westat2000}---in this case the package only allows for executing
  these analyses in R, which may be easier to implement due
  to a syntax related interface. Some methods in WesVar are not implemented in \code{eatRep} yet, for example methods of balanced repeated
  replicates (BRR) or bootstrapping or even JK1. However, some methods are only implemented in \code{eatRep}, for example analyses for nested
  imputed data or linear logistic regression models.\\ 
  
  \code{eatRep} heavily relies on the \code{survey} package \citep{Lumley2012} which functions has been extended by methods 
  for multiple imputed data. While the functional principle of \code{survey} is based on replication of conventional 
  analyses, \code{eatRep} is based on replication of \code{survey} analyses to take multiple imputed data into account.
  %%  The following tutorial does not intend to give theoretical foundations nor a critical review of those methods or of anything else. Also, this is 
%%  not an introduction to R.
\end{abstract}

<<preliminaries,echo=FALSE,print=FALSE>>=
library(fmsb)
library(eatData)
library(eatRep)
@

\section{Introduction}
\label{sec:intro}

In a completely random sample, the mean
\begin{equation}
  \label{eq:1}
  %% \bar x= \frac{\sum\limits_{i=1}^n(x_i)}{n}
  \bar x= n^{-1}\sum\limits_{i=1}^n(x_i)
\end{equation}
is an unbiased estimate for the corresponding mean
\begin{equation}
  \label{eq:2}
  %% \mu= \frac{\sum\limits_{i=1}^N(x_i)}{N}
  \mu= N^{-1}\sum\limits_{i=1}^N(x_i)
\end{equation}
of the underlying population the sample was drawn from. This does not hold for dispersion measures (variance and standard
deviation), as the variance in a sample is always less than the variance in the population the sample was drawn from. The transformation,
however, is very easy made: The variance in a sample is multiplied by $n/(n-1)$ to obtain population variance, where $n$ is the sample size.
Based on
\begin{equation}
  \label{eq:3}
  %% \sigma^2= \frac{\sum\limits_{i=1}^N(x_i-\mu)^2}{N}
  \sigma^2= N^{-1}\sum\limits_{i=1}^N(x_i-\mu)^2
\end{equation}
for the population with $N$ elements, we apply
\begin{equation}
  \label{eq:4}
  %% s^2= \frac{\sum\limits_{i=1}^n(x_i-\bar x)^2}{n-1}
  s^2= (n-1)^{-1}\sum\limits_{i=1}^n(x_i-\bar x)^2
\end{equation}
to estimate population variance from a sample of size $n$. In a weighted sample, i.e. if the population weights differ between
examinees in the sample, mean and variance may be estimated by incorporating these population weights.
(In a completely random sample, these weights equal 1 for each examinee.)
\begin{equation}
  \label{eq:5}
  \bar x_w= \sum\limits_{i=1}^n\left(\dfrac{w_i}{W}x_i\right) ,
\end{equation}
\begin{equation}
  \label{eq:6}
   s_{w}^{2}= \sum\limits_{i=1}^n \dfrac{w_i}{W-1}(x_i-\bar x)^2 ,
\end{equation}
%%\begin{equation}
%%  \begin{aligned}
%%  \label{eq:5}
%%  \bar x_w&= \dfrac{\sum\limits_{i=1}^n\left(\dfrac{w_n}{W}x_i\right)}{W}\\
%%  s^2&= \dfrac{\sum\limits_{i=1}^n \dfrac{w_n}{W-1}(x_i-\bar x)^2}{n} \\
%%  s^2&= n^{-1}\sum\limits_{i=1}^n \frac{w_n}{W-1}(x_i-\bar x)^2 ,
%%  \end{aligned}
%%\end{equation}
where $w_i$ is the case weight of the \emph{i}th person, and $W$ is the sum over all case weights, i.e. $W=\sum w_i$.
To summary, the crucial point in the estimation of population variance estimates is the factor $n/(n-1)$. Unfortunately, this factor only
applies when we sample (conditionally) independently from the population, as in completely random samples or weighted
random samples. In a clustered sample, however, where schools or classes are sampling units instead of single persons,
the relationship between sample and population variance is not so clear at all. The reason is that persons \emph{within} a cluster (for
example pupils in a class) often share a common variance. The sample variance underestimates the population variance, but more
severely than indicated by the factor $n/(n-1)$. To estimate the relationship between sample and population variance, it is
necessary to estimate the variance explained by the cluster.\\

Without taking the cluster structure into account, we would not only obtain biased variance estimates but biased standard errors,
too \citep{luke2009}. This problem occurs in the same way for estimation of frequency tables, quantiles or estimates
of (linear) regression models. To gain unbiased estimates, several replication methods were introduced, which based on the same
principle: To estimate the proportion by which the variance in the sample is underestimated due to a clustered 
structure \citep{Lumley2004}. In the Jackknife-2 (JK2) procedure which is the only procedure implemented in \code{eatRep} to 
date, this is implemented by reproducing the original sample to several replicates. In each replicate one sampling unit (e.g. one class)
is replaced by another class, which therefore occurs two times in the sample. Each replicate is analyzed if it would have been a
completely random sample. Recognize what is to be expected then: If the variance is explained partially by the clusters, removing
one sampling unit should decrease the variance of the sample slighty. Conversely, the point estimates of each replication sample
should vary slightly. The variance in the point estimates between the replicates is used to estimate unbiased parameters.
Otherwise, if there is no variance between clusters, removing one cluster would have no effect on the variance estimate, 
and the point estimates between replicates would have no variance. In this case replication methods will result in exactly 
the same variance estimates and standard errors as they would follow from conventional analysis.\\

For the purpose of illustration, assume a simple population mean which has to be estimated from a completely random sample of $N=1000$.
To estimate the standard error of this mean, we may apply a rather laborious method: to draw 100 samples (with replacement) from our original
sample, each of $N=1000$, and compute the mean in each sample. The standard deviation of the 100 means is the standard error of the mean
estimate. Of course, this bootstrap method is far to cumbersome, as in a random sample the standard error can be estimated in
a much more easier way. However, in a clustered sample, an extension of this bootstrap method is appropriate indeed.
Several software \citep{Westat2000} and free R packages such as \code{survey} \citep{Lumley2012}
do allow for several replication methods.\\

The situation is becoming still more complicated when the variables in the data to be analysed occur as (multiple) 
imputed data, for example as plausible values. Where missing values may cause biased parameters, analyses are conducted with 
imputed data. Often, the original data which includes missing values is reproduced several times, whereas the missing entries 
are filled with a set of plausible values, which results in several imputed data sets. 
To gain unbiased parameter estimates, the analyses are conducted for each data set 
separately and pooled afterwards according to \citet{Rubin1987}.\\

If we have both, a clustered sample with multiple imputed data, both methods have to be combined. This leads to a replication
of replications. Analyses have to be repeated to account for the clustered structure, and the results of these replications 
have to be repeated to account for multiple imputed data. In the following, we refer to "cluster replicates" and "imputation replicates" 
to differentiate between both.

\section{Estimate some population descriptives}
\label{sec:ex1}
In this example, we use some artificial data from the context of educational research. We may think of a stratified clustered sample of 
German fourth-grade primary school students whose reading and writing competencies are measured. Proficiency estimates obtained from a 
Item response Theory (IRT) marginal model are included as plausible values. Each plausible value may be recognized as an imputation 
of the latent competence construct.\\ 

<<reading_writingdef>>=
str(reading_writing)
@

Requesting the data structure provides us with information about the number and type of variables and the number of
examinees. \code{"idstud"} is a unique person identifier of the 4,619 examinees, \code{"wgtSTUD"} a person weight, 
\code{"country"} denotes the country the person comes from. \code{"JKZone"} and \code{"JKrep"} denote jackknifing 
variables which contains information about which unit has to be replaced by which other unit 
in which replicate of the original data. We may think of \code{"reading\_score1"}, \code{"reading\_score2"} 
and \code{"reading\_score3"} as three plausible values for the reading competence. Please note that the three 
imputations of the reading competence occur as three different variables whereas they conceptually belong to one 
common competence measure. This is quite usual if multiple imputed data is presented in a wide-format dataset. 
\code{eatRep} strictly requires the wide format which becomes apparent when dealing with nested multiple imputations. 
Please note further that the dataset does not contain any replicates, only the 
information required for generating them, captured in the \code{"JKZone"} and \code{"JKrep"} variables. 

\subsection{Populations means, standard deviations, variances and mean differences}
\label{subsec:means}

We now want to compute the means by each country, considering the clustered structure as well as the multiple imputed data structure. The replicates
do not need to be created separately, as they will be generated in each analysis automatically. Even in large data sets this takes only a few 
seconds.\\

<<readingMeansByCountrydef>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone", 
         JKrep = "JKrep", groups = list(federalState = "country"), 
         dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
@

One may ask whether the \code{groups} argument has to be specified in this laborious list format. The answer is: If the grouping
variable is multiple imputed as well, its imputations have to be specified in a character vector of corresponding variable names
as well as in the \code{dependent} argument. A statement like \code{groups = list(federalState = c("country1", "country2"))} then
would refer to two imputations of the group variable.\\

While the function is operating, some additional information is displayed on console. First we see that 81 replicate weights are created
due to 81 distinct jackknifing zones in the JKZone variable. This information refers to the "cluster replicates" and implies 
that the subsequent analysis has to be repeated 81 times \emph{for each imputation}. In each of the 81 replication samples, 
one unit (e.g. school) of a certain jackknifing zone is missing and the weights of the other unit of the same zone are 
doubled. The data in all other zones remain unchanged. Each analysis revealed slighty different results. This variation is used
to estimate the sampling variance. But why it is talking about 3 replications overall? This refers to the "imputation replicates",
because because we have defined three plausible values of the dependent variable. Whereas we only change the weights in the
"cluster replicates" and work with identical variables, we change the variables (e.g. the imputations) between the "imputation
replicates". To sum up, for each of the three imputations, 81 analyses are executed, which results in $3\times 81=243$ 
analyses overall. The little dots continuously appearing on the console therefore refer to "imputation replicates" and are 
intended to work as a rough progress bar. Each dot represents one replication. When the procedure finished, the results 
are pooled in the case of more than one imputation.\\

<<readingMeansByCountryResultsdef>>=
means[c(1:4,19:20),]
@

The output is a data frame in the long format with 30 rows. To keep the overview, only a few selected rows are displayed here. 
For each subpopulation denoted by the \code{groups} statement (here: \code{LandA}, \code{LandB} and \code{LandC}), each 
dependent variable (here: only the reading competence), each parameter (we requested mean, variance, standard deviation and
sample size or population size) and each coefficient (i.e., the estimate and the corresponding standard error) the 
corresponding value is given. To display the results in the more common wide format, use the \code{reshape2} package. 
A possible call would be \code{reshape2::dcast(means, group ~ parameter+coefficient, value.var = "value")}. Alternatively, 
an abbreviated display of the results is provided by the \code{dM} function (whereas dM stands for "display means").
You may think of \code{dM} as a simple summary function which is not intended for saving results or further processing
as the results are displayed in an abbreviated (i.e., rounded) manner to offer clear arrangement on console.
The \code{dM} function has an additional argument to omit displaying parameters or coefficients you are
not interested at the moment. \\

<<readingMeansByCountryResultsdefSummary>>=
dM(means, omitTerms = c("var", "Ncases","NcasesValid", "meanGroupDiff") )
@

Is it possible to see how the results would change if we do not consider the clustered structure? Sure is is! 
Simply leave out the jackknifing arguments \code{JKZone} and \code{JKrep}. The results will be pooled only due to 
multiple imputed data:\\

<<readingMeansByCountry2def>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", 
      groups = list(federalState = "country"), 
	  dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
dM(means, omitTerms = c("var", "Ncases","NcasesValid", "meanGroupDiff") )
@

We see that the means are completely unaffected, but the standard deviation now is lower. Consequently, also 
the standard errors for the mean estimates are considerably lower. (Standard errors for standard deviations and for 
variances are not implemented yet.) If we decide to leave out the weights as well, we would additionally expect to receive 
different means now:\\

<<readingMeansByCountry3def>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud", groups = list(federalState = "country"), 
	  dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
dM(means, omitTerms = c("var", "Ncases","NcasesValid", "meanGroupDiff") )
@

Two possible interesting features should be emphasized in the following. First assume that we do not have one, but two
grouping variables, namely \code{federalState} and \code{gender}. As we have three federal states and two gender values, 
the whole population is splitted into $3\times 2=6$ subpopulations for which descritives can be requested. If we 
additionally are interested in the descriptives of the whole population or the descriptives \emph{within each state, but together for both gender groups},
we can use the \code{group.splits} argument to particularly specify the groups we are interested in. Let us consider for 
example the two group variables \code{federalState} and \code{gender}. If \code{group.splits} equals 2 (the default, i.e., 
the number of grouping variables), descriptives for the $3\times 2=6$ subpopulations are computed. If \code{group.splits} 
is \code{1:2}, descriptives for each state (e.g., across gender) and each gender group (e.g. across federal states) 
additionally are computed. If \code{group.splits} is \code{0:2}, descriptives also for the whole population 
(e.g. across gender \emph{and} federal states) are computed. \\

The second feature is about mean differences. Suppose you are interested in the gender difference \emph{within} each 
federal state. The grouping variable for which mean differences should be computed has to be specified in 
the \code{group.differences.by} argument. For a grouping variable with $K$ levels, all $K!/(2!*(K-2)!)$ comparisons
are computed. It is important that the group defined in \code{group.differences.by} also has to occur in the \code{groups} 
statement, otherwise \code{group.differences.by} will be ignored. To estimate gender differences \emph{within each 
federal state}, gender and federal state have to be part of the \code{groups} statement, whereas only gender has to be
used in the \code{group.differences.by} argument. Both features are illustrated in the following example:\\

<<readingMeansByCountry4def>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone", 
         JKrep = "JKrep", groups = list(federalState = "country", gender = "sex"), 
         group.splits = c(0,2), group.differences.by = "gender", 
	  dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
@

First note the \code{group.splits} is set to \code{c(0,2)}, which means that we request descriptives for the whole 
population and the 6 subpopulations. Consequently, two analyses are conducted. The \code{group.differences.by} only
applies for the second analysis, as the gender group is not considered relating to the whole population analysis. 
Please note that you have to use the name of the group variable, not the group variable itself: 
\code{group.differences.by = "gender"} instead of \code{group.differences.by = "sex"}! To estimate gender differences 
\emph{across all federal states}, only gender has to be part of the \code{group} statement, and only gender has to be
used in the \code{group.differences.by} argument. The output of the analysis is nearly the same as we would have omitted 
the \code{group.differences.by} argument, but now, some additional lines have joined. Again, we may use the \code{dM}
function to display the part of the results we are interested in---note that now we do \emph{not} 
exclude \code{meanGroupDiff} from the results to summarize:\\

<<readingMeansByCountry5def>>=
dM(means, omitTerms = c("var","Ncases", "NcasesValid"))
@

The output now changed slightly: Instead of several group columns, only one column for group membership is provided. 
The last line labelled \code{wholeGroup} provides results concerning the whole population. The line labelled 
\code{LandC\_male} contains values for the males in LandC. Moreover, three mean differences were computed. In each
federal state, the difference between males and females is given. \\

\subsection{Frequency tables}
\label{subsec:freqs}

Computation of frequency tables works in the same manner as in the examples mentioned before. Representative for several possible analyses
only one example is given below. First of all, let's have a look at the backmost columns in our example data:\\

<<readingWriting2def>>=
reading_writing[1:5,19:23]
@

The Hisei variables \code{"zehisei1"}, \code{"zehisei2"}, \code{"zehisei3"}, \code{"zehisei4"} and \code{"zehisei5"} indicates 
high versus low socio-economical status which is clustered in five groups. Hence, each variable consists of five distinct 
values. Categorical variables are often represented as factors in R, which is quite straightforward. 
However, the \code{"zehisei"} variables are of class numeric. This is an inconsistency which may cause annoying 
misinterpretations when such variables are called in functions related to the generalized
linear model like \code{aov()}, \code{glm()} etc. For the computations of frequency tables it is not necessary to convert the
variable class to factor.\\ 

We now are interested in the relative frequencies of this groups in the different countries and within each country 
for different groups of gender. As before, we want to take the cluster structure and multiple imputations into account.\\

<<hiseiFreqsByCountryGenderdef>>=
freqs <- jk2.table(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", 
      JKZone = "JKZone", JKrep = "JKrep", 
	  groups = list(federalState = "country", gender = "sex"), 
	  dependent = list(Hisei = paste("zehisei",1:5,sep="")))
dT(freqs)
@

The output is a single data frame in the long format. To make the output more
pleasing to the eye, a short summary function \code{dT} is just waiting to do her job, to summarize the results.
For each gender group in each country, the relative frequency
of each Hisei category is given with its standard error. The first column refers to the dependent variable for which we 
want to compute frequencies. The next two columns refer to the groups specified in the analysis 
(in our example: \code{federalState} and \code{gender}). The ``labels'' of the dependent variable
now are captured in the column names of the summary table. We see that in federal 
state \code{"LandA"} the first Hisei category labelled \code{"1"} has a relative frequency of about 4.2 percent 
in the female group and 3.7 percent in the male group.\\ 

As in the examples mentioned before, these analyses may be conducted without considering clustered structure. 
See whether the standard errors will change. Technically, this function even works when the results are theoretically 
questionable. First, let's imagine we have no "5" -- values in the first and second imputation of the Hisei:\\

<<redefineValues1def>>=
reading_writing2 <- reading_writing
reading_writing2[which(reading_writing2[,"zehisei1"] == 5),"zehisei1"] <- 4
reading_writing2[which(reading_writing2[,"zehisei2"] == 5),"zehisei2"] <- 4
@

Here, we replaced the "5" values by "4" values. Moreover, we assume missings on the Hisei imputations 2, 3 and 4.
Missings on an imputed variable? Yes, for example, when we want to estimate the proportion of missingness. 
(Conceptually, however, it makes more sense to define the categories by certain categories, for example
\code{c("hisei\_group1", "hisei\_group2", "hisei\_group3", "hisei\_group4", "hisei\_group5", "no\_anwer")}. )\\

<<redefineValues2def>>=
cols <- paste("zehisei", 2:4, sep = "")
for (i in cols) {
     casesToNA <- sample(x = c(1:nrow(reading_writing2)), size = 12, replace = FALSE )
     reading_writing2[ casesToNA ,i ] <- NA
}
@

For each of the Hisei imputations 2, 3 and 4, we choose 12 random cases to insert a missing value. Will the function have to
surrender when the data structure is of this kind?\\

<<redefineValuesdef>>=
freqs2 <- jk2.table(dat = reading_writing2, ID = "idstud", wgt = "wgtSTUD",
      separate.missing.indikator = TRUE, JKZone = "JKZone", JKrep = "JKrep", 
	  groups = list(federalState = "country", gender = "sex"), 
	  dependent = list(Hisei = paste("zehisei",1:5,sep="")))
dT(freqs2)
@	  

Without bothering you unduly with the output, I only want to mention two details: First a new category has joined to the output,
which is labelled \code{"missing"}. Secondly, remember what we have done with our data disturbance. We replaced the "5" values
from the first and second imputation. Now, as group "5" no longer appears in both imputations, the distribution of the categories
now vary more widely between imputations. In the concept of multiple imputation this reflects the uncertainty due to missing 
data. Consequently, the standard errors especially for group "5" now have grown up, compared to the preceding analysis.\\ 

\subsection{Quantiles}
\label{subsec:quantiles}

Estimation of quantiles for numerical variables is possible using the function \code{jk2.quantile}. All related analyses 
mentioned up to this point apply in the same way. Note that these analyses apply for numerical dependent variables. 
See the examples in the help file of \code{jk2.quantile()}.\\

\section{Generalized linear models}
\label{sec:ex2}

Considering multiple imputations and clustered structure in the estimation of generalized linear models is based on the same 
principes mentioned before. However, some additional comments due to specific characteristics of regression models have to be 
made. First we now have another type of variable---independent variables, which may occur as multiple imputed variables, too.
Second, we (optionally) have to specify the regression expression, what about we may are confused, as one variable occurs 
in different labels in multiple imputed data sets. Third, we will have to specify the kind of regression we propose to estimate, 
for example linear or logistic regression. We start with a simple example using the same data as before.\\

<<regression1def>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", groups = list(country = "country"),
         dependent = list(reading = paste("reading_score", 1:3, sep = ""),
                          writing = paste("writing_score", 1:3, sep = "")),
         independent = list(gender = "sex", INCOME = c("income1", "income2")),
         complete.permutation = "no", glm.family = gaussian(link="identity") )
@

As we might have expected, the outcome is a single data frame in the long format. And long really means long!
For our purpose, it may be sufficient to content ourself with the summary provided by \code{dG}. But beforehand let us 
consider how many regression analyses are conducted and how many results we expect to find. The message on the console 
speaks of about "2 analyses overall" according to two dependent variables, reading and writing. But strictly speaking, 
we have estimated six regression analyses, as the model is fitted in each
group separately. As we specified one group variable dividing the data into three distinct groups, for which we instruct
\code{jk2.glm()} to fit the regression model separately, we find results of the three models for each of the two dependent 
variables in the results. More specifically, for each country, an intercept and two regression coefficients according 
to \code{gender} and \code{INCOME} are estimated. The \code{dG} function allows us to have a look only
at a specific result out of the 6 analyses. \code{analyses = 1:2} advises the function to display the results of the 
first and second analysis. First we should consider that each single analyses is characterized by two variables, 
the group for which the model is fitted, and the dependent variable. In the heading we find information about both. 
The actual regression results are displayed underneath. \\

<<regression1defResults1>>=
dG(mod1, analyses = 1:2)
@

Remember what was said about factors in the chapter about frequency tables: The gender variable now
has to be defined explicitly to be of class factor! Otherwise, albeit gender variable may be coded as 0/1, it would be treated to be a
continuous numeric variable. With only two levels---male and female---this may have no effect on the results, but consider a factor
variable with three levels, which may be coded 0, 1 and 2. We are interested in two coefficients which correspond
to the effect of level 1 vs. level 0 and the effect of level 2 vs. level 0. If we miss to define the variable to be of class factor,
only one coefficient is computed, and the variable is assumed to be continuous.
What we see additionally is that R implicitly defined the female group to be the reference---the
regression parameter was labelled \code{gendermale}.\\ 

Now we try something different. First we define gender to be our dependent variable. Secondly, we use country as a predictor. 
This is to test whether the proportions of gender vary between countries. To simplify displaying the results, we use the same
workaround as in the example before. \\

<<regression2def>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", dependent = list(gender = "sex"),
         independent = list(federalState = "country") ,
         complete.permutation = "no", glm.family = binomial(link="logit") )
dG(mod1)
@

As we have no imputed variables, only one replication is run. No pooling has taken place. Although we have only defined one
independent variable, we obtain two regression coefficients for the two categories of the country variable. Again, R choosed 
its favorite reference group by itself. The effects are expressed in relation to \code{LandA}. To interpretate the effects, 
the coefficients may be transformed to odds ratios:\\

<<transformdef>>=
exp(mod1[3:5,"value"])
@

In \code{LandB} the odds ratio to be male is 0.93 times the corresponding odds ratio in \code{LandA}. The following subsections
address three little questions one might ask oneself.

\subsection{How to change reference group at costumer's option}
\label{subsec:faq1}

As we saw in the preceding section, R choosed the reference group of factor variables by itself. Persuading R to meet our needs
is easier said than done. The essentially easiest way is a rather dummy method: recode the variable so that your favourite
reference group occurs at first when ordered alphabetically. We will demonstrate this procedure about the gender variable in our
fictitious data set. Remember the first example in section 3---R choosed the females to be the reference. Why? Simply because
female comes before male in the alphabet. Let's use the \code{recode()} function from the \code{car} package to define a new
variable \code{sexRecoded}:\\

<<recodeSexdef>>=
library(car)
reading_writing[,"sexRecoded"] <- car::recode(reading_writing[,"sex"], "'male' = '_male'")
@

The simple trick of adding a \code{"\_"} sign to the \code{"male"} label provokes R to sort \code{"\_male"}
before \code{"female"} alphabetically. Consequently, \code{"male"} is used as reference group when repeating the first example 
analysis of section 3.\\

<<regressionReplicationdef>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", groups = list(country = "country"),
         dependent = list(reading = paste("reading_score", 1:3, sep = "")),
         independent = list(gender = "sexRecoded", INCOME = c("income1", "income2")),
         complete.permutation = "no", glm.family = gaussian(link="identity") )
dG(mod1, analyses = 1)
@

\subsection{How to modify the regression statement}
\label{subsec:faq2}

When we call \code{glm()}, we are advised to specify the formula statement of the regression model. In \code{jk2.glm()}, however,
the regression statement is created automatically by the independent variables connected by a \code{"+"} symbol. As \code{jk2.glm()}
is advised to accomplish multiple imputations, the names of one and the same variable change between imputations. Consequently,
the regression statement is adapted between replications. However, regression models which go beyond simple addition of predictors,
have to be specified manually. The right side of the regression formula has to defined via the \code{reg.statement} argument 
as a string, whereas the
names of the independent variables occurring in the \code{independent} argument have to be used. We illustrate this procedure by
seizing on the preceding example. We have two independent variables, \code{gender} and \code{INCOME}. Without specifying any 
\code{reg.statement} argument, \code{jk2.glm()} implicitly uses \code{gender + INCOME}. Modelling an additional 
interaction of both variables needs to be defined explicitly:\\

<<regressionReplicationInteractiondef>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", groups = list(country = "country"),
         dependent = list(reading = paste("reading_score", 1:3, sep = "")),
         independent = list(gender = "sexRecoded", INCOME = c("income1", "income2")),
         reg.statement = "gender * INCOME",
         complete.permutation = "no", glm.family = gaussian(link="identity") )
@

Please not that the regression statement---if specified---must contain all variables defined in the \code{independent} argument.
Note further that the names of the variables occurring in the \code{independent} argument are used instead of single imputation 
variable names. Don't try your luck with \code{reg.statement = "sexRecoded * income1"}! This logic implies that the list of
variables in the \code{independent} argument has to be named. Consequently, \code{independent = list("sexRecoded", c("income1", "income2"))}
will only work as long as no \code{reg.statement} argument is specified. Let's have a look at the output of the first analysis:\\

<<regressionReplicationInteractionOutputdef>>=
dG(mod1, analyses = 1)
@

\subsection{Which of both determination coefficients should I pay attention?}
\label{subsec:faq3}

The output of each \code{jk2.glm()} analysis also contains the pooled determination coefficient, $R^2$, most frequently 
the conventional $R^2$ and Nagelkerke's $R^2$. However, in linear regression models, i.e. if the identity link is used, 
assuming normally distributed errors, the conventional $R^2$ should be used to interpret explained variance. In log-linear
regression models, i.e. if the binomial link function is used, Nagelkerke's $R^2$ should be used. The reason for reporting 
both coefficients is the programming disability of the package developer. 

\section{Nested imputations}
\label{sec:nested}

The next to last chapter of this little tutorial is reserved to the problem of nested imputation. The general concept is described in
\citet{Rubin2003}. At this point, only some specific aspects which are relevant in large scale assessments, are mentioned. Suppose you
want to estimate IRT proficiencies (often denoted $\theta$) in a specific domain. Applying an extensive marginal model which 
comprehends of items responses and background information as well, the posterior distribution of each examinees' $\theta$ is 
specified. Without any certain proficiency value of a specific examinee, plausible values 
are drawn from the posterior of each examinee. Conceptually, plausible values are multiple imputations of the inherently missing 
variable $\theta$ and may analyzed in standard statistic procedures according to the generalized linear model. To obtain valid
estimates and standard errors, the results have to be pooled according to \citet{Rubin1987}.\\

Suppose you have missing data in the background variables as well, which have to be imputed in the first step, which may result in 
$M=5$ data sets. For each data set a marginal IRT model is specified and $N=20$ plausible values are drawn. Overall 
$5\times 20=100$ plausible values in a dependency structure will result from the analysis. Formally, we now have nested imputed 
data. To pool the results, the formulas in \citet{Rubin1987} cannot
be applied, as the plausible values do not stem from a common ``nest''. The interdependence has to be taken into account. 
Whereas the conventional pooling formulas split the overall variance in the variance within imputation and the variance between 
imputation, where the latter one is used to estimate the uncertainty due to imputation, the formulas for nested imputation 
extend the old ones by splitting the variance between imputation in the within-nest variance and the variance between nests. 
See \citet{Rubin2003} for further details. These varied formulas are also implemented in \code{eatRep}.\\

If the data analysed with \code{eatRep} stem from a nested multiple imputation structure, this structure has to be specified. 
More specifically, \code{eatRep} has to know the number of nests and the number of imputations in each nest. Moreover, it has
to be specified which variable belongs to which imputation in which nest. The above procedure sounds more complicated than it 
hopefully is.\\

\subsection{Example: Compute descriptives from a nested imputation structure}
\label{subsec:nest1}

The \code{reading\_writing} data set ist not very proper for instructional purposes concerning nested imputations, but we will do 
our best. First consider the variables \code{"income1"} and \code{"income2"}. Suppose that these are only two imputations of
an \code{INCOME} variable which originally contains missing values. Therefore, we have $M=2$ nests. Now we assume that we have 
measured only one domain in our IRT analysis, which we call ``ability''. Suppose we draw three plausible values in each 
nest, therefore $N=3$. The corresponding variables are \code{"reading\_score1"}, \code{"reading\_score2"} and 
\code{"reading\_score3"} for the first nest, and \code{"writing\_score1"}, \code{"writing\_score2"} and 
\code{"writing\_score3"} for the second nest. (I am aware that the variable labelling is not disarmingly intuitive that way.)
Overall, we have $3\times 2=6$ plausible values in a dependency structure. We want to estimate descriptives for groups of
high vs. low income, and for each country separately. First we create an indicator of high vs. low income, where we set the 
threshold arbitrarily at 2000.\\

<<nestEx1def>>=
reading_writing[,"income_ind1"] <- factor(ifelse(reading_writing[,"income1"]>2000,"high","low"))
reading_writing[,"income_ind2"] <- factor(ifelse(reading_writing[,"income2"]>2000,"high","low"))
@

We than will have to specify two groups for which we want to compute descriptives. Remember that one group appears as a nested 
imputed variable:\\

<<nestEx2def>>=
means  <- jk2.mean(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", 
          JKZone = "JKZone", JKrep = "JKrep",
          groups = list(country = "country", INCOME = c("income_ind1","income_ind2")),
          dependent = list(ability = list(nest1 = paste("reading_score",1:3,sep=""),
                                          nest2 = paste("writing_score",1:3,sep="") )),
          complete.permutation = "no" )
@

The nested structure has to be specified in the \code{dependent} and \code{group} argument. The \code{dependent} argument now 
is no longer a list of character vectors but a list of lists. As \code{dependent} is a list of length 1, only one analysis is 
conducted. As the first and only element of \code{dependent} is a list instead of a character vector, a nested imputation 
structure is 
implied. As the length of the first element of \code{dependent} is 2, $M=2$ nests are assumed. Therefore, the function expects
to find group variables either without any imputations or with exactly $M=2$ imputations. Moreover, the plausible values of the 
first nest (labelled \code{"reading\_score1"}, \code{"reading\_score2"} and \code{"reading\_score3"}) are assumed to belong to
the first imputation of \code{INCOME}, labelled \code{"income1"}. Correspondingly, the plausible values of the 
second nest (labelled \code{"writing\_score1"}, \code{"writing\_score2"} and \code{"writing\_score3"}) are assumed to belong to
the second imputation of \code{INCOME}, labelled \code{"income2"}.\\

Conceptually, the \code{complete.permutation} argument should be set to \code{"nothing"} if a nested imputation structure is 
assumed. The output reveals rather high standard errors, as in our fictitious example the between-nest variance is rather high.\\

<<nestEx2Resultsdef>>=
dM(means, omitTerms = c("var", "Ncases", "NcasesValid", "meanGroupDiff"))
@

\subsection{Example: Fit a linear regression model in a nested imputation structure}
\label{subsec:nest2}

The principles of considering the nested structure are quite the same as in the preceding example. For each country and the both
\code{income} groups we want to predict ``ability'' by \code{gender} and \code{INCOME}. Using \code{income} as group variable likewise
allows for investigating whether a potential effect of \code{INCOME} on ``ability'' differs in groups of high vs. low income.
Additionally, we model a possible interaction between \code{gender} and \code{INCOME}.\\

<<nestEx3def>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD",
          JKZone = "JKZone", JKrep = "JKrep",
          groups = list(country = "country", INCOME.group = c("income_ind1","income_ind2")),
          dependent = list(ability = list(nest1 = paste("reading_score",1:3,sep=""),
                                          nest2 = paste("writing_score",1:3,sep="") )),
          independent = list ( gender = "sex", INCOME = c("income1", "income2")),
          reg.statement = c("gender * INCOME"), glm.family = gaussian(link = "identity"),
          complete.permutation = "no" )
dG(mod1, analyses = 1)
@

\bibliography{eatrep}

\end{document}
