\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{compute descriptives of data structures (``designs'')}
%%\VignetteDepends{eatDesign}
%%
\newcommand{\trans}{\ensuremath{^\mathsf{T}}}
\newcommand{\invtrans}{\ensuremath{^\mathsf{-T}}}
\title{eatRep: a package to analyze multiple imputed data in complex survey designs}
\author{Sebastian Weirich\\Humboldt University Berlin, Germany}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=true,keep.source=TRUE}
\SweaveOpts{include=FALSE}
\setkeys{Gin}{width=\textwidth}
\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\package}[1]{\textsf{\small{#1}}}
\maketitle
\begin{abstract}
  Estimation of simple descriptive statistics becomes cumbersome, if the sample cannot be considered to be a (completely) random draw
  from the population for which descriptives should be interpreted. This occurs in weighted samples or clustered samples. The same is true
  if the variables of interest stem from a multiple imputation process and occur, for example, as plausible values.
  This tutorial describes some basic analyses to compute descriptives in complex survey designs using the R package \code{eatRep}, which was
  designed mainly to supply replications methods in R. To date, only the Jackknife-2 (JK2) method is supported. Some functions overlap with 
  methods provided in the computer software WesVar \citep{Westat2000}---in this case the package only allows for executing
  these analyses in R, which may be easier to implement due
  to a syntax related interface. Some methods in WesVar are not implemented in \code{eatRep} yet, for example methods of balanced repeated
  replicates (BRR) or bootstrapping or even JK1. However, some methods are only implemented in \code{eatRep}, for example analyses for nested
  imputed data or linear logistic regression models.\\ 
  
  \code{eatRep} heavily relies on the \code{survey} package \citep{Lumley2012} which functions has been extended by methods 
  for multiple imputed data. While the functional principle of \code{survey} is based on replication of conventional 
  analyses, \code{eatRep} is based on replication of \code{survey} analyses to take multiple imputed data into account.
  %%  The following tutorial does not intend to give theoretical foundations nor a critical review of those methods or of anything else. Also, this is 
%%  not an introduction to R.
\end{abstract}

<<preliminaries,echo=FALSE,print=FALSE>>=
library(fmsb)
library(eatData)
library(eatRep)
@

\section{Introduction}
\label{sec:intro}

In a completely random sample, the mean
\begin{equation}
  \label{eq:1}
  %% \bar x= \frac{\sum\limits_{i=1}^n(x_i)}{n}
  \bar x= n^{-1}\sum\limits_{i=1}^n(x_i)
\end{equation}
is an unbiased estimate for the corresponding mean
\begin{equation}
  \label{eq:2}
  %% \mu= \frac{\sum\limits_{i=1}^N(x_i)}{N}
  \mu= N^{-1}\sum\limits_{i=1}^N(x_i)
\end{equation}
of the underlying population the sample was drawn from. This does not hold for dispersion measures (variance and standard
deviation), as the variance in a sample is always less than the variance in the population the sample was drawn from. The transformation,
however, is very easy made: The variance in a sample is multiplied by $n/(n-1)$ to obtain population variance, where $n$ is the sample size.
Based on
\begin{equation}
  \label{eq:3}
  %% \sigma^2= \frac{\sum\limits_{i=1}^N(x_i-\mu)^2}{N}
  \sigma^2= N^{-1}\sum\limits_{i=1}^N(x_i-\mu)^2
\end{equation}
for the population with $N$ elements, we apply
\begin{equation}
  \label{eq:4}
  %% s^2= \frac{\sum\limits_{i=1}^n(x_i-\bar x)^2}{n-1}
  s^2= (n-1)^{-1}\sum\limits_{i=1}^n(x_i-\bar x)^2
\end{equation}
to estimate population variance from a sample of size $n$. In a weighted sample, i.e. if the population weights differ between
examinees in the sample, mean and variance may be estimated by incorporating these population weights.
(In a completely random sample, these weights equal 1 for each examinee.)
\begin{equation}
  \label{eq:5}
  \bar x_w= \sum\limits_{i=1}^n\left(\dfrac{w_i}{W}x_i\right) ,
\end{equation}
\begin{equation}
  \label{eq:6}
   s_{w}^{2}= \sum\limits_{i=1}^n \dfrac{w_i}{W-1}(x_i-\bar x)^2 ,
\end{equation}
%%\begin{equation}
%%  \begin{aligned}
%%  \label{eq:5}
%%  \bar x_w&= \dfrac{\sum\limits_{i=1}^n\left(\dfrac{w_n}{W}x_i\right)}{W}\\
%%  s^2&= \dfrac{\sum\limits_{i=1}^n \dfrac{w_n}{W-1}(x_i-\bar x)^2}{n} \\
%%  s^2&= n^{-1}\sum\limits_{i=1}^n \frac{w_n}{W-1}(x_i-\bar x)^2 ,
%%  \end{aligned}
%%\end{equation}
where $w_i$ is the case weight of the \emph{i}th person, and $W$ is the sum over all case weights, i.e. $W=\sum w_i$.
To summary, the crucial point in the estimation of population variance estimates is the factor $n/(n-1)$. Unfortunately, this factor only
applies when we sample (conditionally) independently from the population, as in completely random samples or weighted
random samples. In a clustered sample, however, where schools or classes are sampling units instead of single persons,
the relationship between sample and population variance is not so clear at all. The reason is that persons \emph{within} a cluster (for
example pupils in a class) often share a common variance. The sample variance underestimates the population variance, but more
severely than indicated by the factor $n/(n-1)$. To estimate the relationship between sample and population variance, it is
necessary to estimate the variance explained by the cluster.
Without taking the cluster structure into account, we would not only obtain biased variance estimates but biased standard errors,
too \citep{luke2009}. This problem occurs in the same way for estimation of frequency tables, quantiles or estimates
of (linear) regression models. To gain unbiased estimates, several replication methods were introduced, which based on the same
principle: To estimate the proportion by which the variance in the sample is underestimated due to a clustered 
structure \citep{Lumley2004}. Technically,
this is implemented by reproducing the original sample to several replicates. In each replicate one sampling unit (e.g. one class)
is replaced by another class, which therefore occurs two times in the sample. Each replicate is analyzed if it would have been a
completely random sample. Recognize what is to be expected then: If the variance is explained partially by the clusters, removing
one sampling unit should decrease the variance of the sample slighty. Conversely, the point estimates of each replication sample
should vary slightly. The variance in the point estimates between the replicates is used to estimate unbiased parameters.
Otherwise, if there is no variance between clusters, removing one cluster would have no effect on the variance estimate, 
and the point estimates between replicates would have no variance. In this case replication methods will result in exactly 
the same variance estimates and standard errors as they would follow from conventional analysis. 
Several software \citep{Westat2000} and free R packages such as \code{survey} \citep{Lumley2012} 
do allow for several replication methods.\\

The situation is becoming still more complicated when the variables in the data to be analysed occur as (multiple) 
imputed data, for example as plausible values. Where missing values may cause biased parameters, analyses are conducted with 
imputed data. Often, the original data which includes missing values is reproduced several times, whereas the missing entries 
are filled with a set of plausible values, which results in several imputed data sets. 
To gain unbiased parameter estimates, the analyses are conducted for each data set 
separately and pooled afterwards according to \citet{Rubin1987}.\\

If we have both, a clustered sample with multiple imputed data, both methods have to be combined. 

\section{Estimate some population descriptives}
\label{sec:ex1}
In this example, we use some artificial data from the context of educational research. We may think of a stratified clustered sample of 
German fourth-grade primary school students whose reading and writing competencies are measured. Proficiency estimates obtained from a 
Item response Theory (IRT) marginal model are included as plausible values. Each plausible value may be recognized as an imputation 
of the latent competence construct.\\ 

<<reading_writingdef>>=
reading_writing[1:5,1:9]
@

Whereas this is a large dataset with more than 4,000 observations, we have a look only at the first five cases and the first nine columns. 
\code{"idstud"} is a person identification number, \code{"wgtSTUD"} a person weight, \code{"country"} denotes the country the person comes 
from. \code{"JKZone"} and \code{"JKrep"} denote jackknifing variables which contains information about which unit has to be replaced by which other unit 
in which replicate of the original data. We may think of \code{"reading\_score1"}, \code{"reading\_score2"} and \code{"reading\_score3"} as three 
plausible values for the reading competence. The data set does not contain any replicates, only the information required for generating
them. 

\subsection{Populations means, standard deviations, variances and mean differences}
\label{subsec:means}

We now want to compute the means by each country, considering the clustered structure as well as the multiple imputed data structure. The replicates
do not need to be created separately, as they will be generated in each analysis automatically. Even in large data sets this takes only a few 
seconds.\\

<<readingMeansByCountrydef>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone", 
      JKrep = "JKrep", group = list(federalState = "country"), 
	  dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
@

One may ask whether the \code{group} argument has to be specified in this laborious list format. The answer is: If the grouping
variable is multiple imputed as well, its imputations have to be specified in a character vector of corresponding variable names
as well as in the \code{dependent} argument. A statement like \code{group = list(federalState = c("country1", "country2"))} then
would refer to two imputations of the group variable.\\

While the function is operating, some additional information is displayed on console. First we see that 81 replicate weights are created
due to 81 distinct jackknifing zones in the JKZone variable. In fact, this means that the subsequent analysis have to be repeated 81 times,
each with a slightly modified weighting variable---remember that one jackknifing zone with two cluster units (e.g. schools) is choosen,
the weights of one unit are set to zero, the weights of the other unit are doubled, whereas the weights of all other cluster units in all
other jackknifing zones remain unchainged. Each analysis revealed slighty different results. This variation is used
to estimate the sampling variance. But why it is talking about 3 replications overall? This is because we have defined three
imputations (e.g. three plausible values) of the dependent variable. In fact, for each of the three imputations, 81 analyses are
executed, which results in $3\times 81=243$ analyses overall. The little dots continuously appearing on the console are intended to work as
a rough progress bar. Each dot represents one replication. When the procedure finished, the results are pooled.\\

<<readingMeansByCountryResultsdef>>=
means
@

The output is a list of data frames, containing the overall number of observations (unweighted and weighted), the mean,
standard deviation and variance, each with an estimated standard error.
Is it possible to see how the results would change if we do not consider the clustered structure? Sure is is! Simply leave out
the jackknifing arguments \code{JKZone} and \code{JKrep}:\\ 

<<readingMeansByCountry2def>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", 
      JKrep = "JKrep", group = list(federalState = "country"), 
	  dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
means	  
@

We see that the means are completely unaffected, but the standard deviation now is lower. Consequently, also the standard errors for the mean
estimates are lower. (Standard errors for standard deviations and for variances are not implemented yet.) If we decide to leave out the weights 
as well, we would additionally expect to receive different means now:\\

<<readingMeansByCountry3def>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud", 
      JKrep = "JKrep", group = list(federalState = "country"), 
	  dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
means	  
@

A possibly interesting feature is to compute mean differences. The group for which mean differences should be computed has to be specified
in the \code{group.differences.by} argument. To date, only groups with exact two levels (for example gender with its two levels males and
females) are allowed. It is important that the group defined in \code{group.differences.by} also has to occur in the \code{group} statement.
To estimate gender differences \emph{within each federal state}, gender and federal state has to be part of the \code{group} statement,
whereas only gender has to be used in the \code{group.differences.by} argument:\\

<<readingMeansByCountry4def>>=
means <- jk2.mean(dat = reading_writing, ID = "idstud",
      JKrep = "JKrep", group = list(federalState = "country", gender = "sex"),
      group.differences.by = "gender",
	  dependent = list(reading = c("reading_score1", "reading_score2", "reading_score3")))
@

Please note that you have to use the name of the group variable, not the group variable itself: \code{group.differences.by = "gender"}
instead of \code{group.differences.by = "sex"}! To estimate gender differences \emph{across each federal state}, only gender has to
be part of the \code{group} statement, and only gender has to be used in the \code{group.differences.by} argument. The output of the
analysis is exactly the same as we would have omitted the \code{group.differences.by} argument. The results of this additional analysis are
captured in the attributes of the results. (This solution if far from elegant and may be improved another time.)\\

<<readingMeansByCountry5def>>=
attr(means[[1]], "difference")
@

\subsection{Frequency tables}
\label{subsec:freqs}

Computation of frequency tables works in the same manner as in the examples mentioned before. Representative for several possible analyses
only one example is given below. First of all, let's have a look at the backmost columns in the data frame:\\

<<readingWriting2def>>=
reading_writing[1:5,19:23]
@

The Hisei variables \code{"zehisei1"}, \code{"zehisei2"}, \code{"zehisei3"}, \code{"zehisei4"} and \code{"zehisei5"} indicates 
high versus low socio-economical status which is clustered in five groups. Hence, each variable consists of five distinct 
values. Categorical variables are often represented as factors in R, which is quite straightforward. 
However, the \code{"zehisei"} variables are of class numeric. This is an inconsistency which may cause annoying 
misinterpretations when such variables are called in functions related to the generalized
linear model like \code{aov()}, \code{glm()} etc. For the computations of frequency tables it is not necessary to convert the
variable class to factor.\\ 

We now are interested in the relative frequencies of this groups in the different countries and within each country 
for different groups of gender. As before, we want to take the cluster structure and multiple imputations into account.\\

<<hiseiFreqsByCountryGenderdef>>=
freqs <- jk2.table(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", 
      JKZone = "JKZone", JKrep = "JKrep", 
	  group = list(federalState = "country", gender = "sex"), 
	  dependent = list(Hisei = paste("zehisei",1:5,sep="")))
freqs	  
@

The output is a list of data frames in the long format. For each gender group in each country, the relative frequency
of each Hisei category is given with its standard error. The columns in each data frame refer to the groups specified in the
analysis (in our example: \code{federalState} and \code{gender}). An additional column labelled \code{suffix} refers to
the ``labels'' of the dependent variable. Based on three federal states, two gender values and five Hisei levels, 
the output data frame has $3\times 2\times 5=30$ lines. We see that in federal state \code{"LandA"} the first Hisei category 
labelled \code{"1"} has a relative frequency of about 4.2 percent.\\ 

As in the examples mentioned before, these analyses may be conducted without considering clustered structure. 
See whether the standard errors will change ...\\ 

Technically, this function even works when the results are theoretically questionable. First, let's imagine we have no "5" --
values in the first and second imputation of the Hisei:\\

<<redefineValues1def>>=
reading_writing2 <- reading_writing
reading_writing2[which(reading_writing2[,"zehisei1"] == 5),"zehisei1"] <- 4
reading_writing2[which(reading_writing2[,"zehisei2"] == 5),"zehisei2"] <- 4
@

Here, we replaced the "5" values by "4" values. Moreover, we assume missings on the Hisei imputations 2, 3 and 4.
Missings on an imputed variable? Yes, for example, when we want to estimate the proportion of missingness. 
(Conceptually, however, it makes more sense to define the categories by certain categories, for example
\code{c("hisei\_group1", "hisei\_group2", "hisei\_group3", "hisei\_group4", "hisei\_group5", "no\_anwer")}. )\\

<<redefineValues2def>>=
cols <- paste("zehisei", 2:4, sep = "")
for (i in cols) {
     casesToNA <- sample(x = c(1:nrow(reading_writing2)), size = 12, replace = FALSE )
     reading_writing2[ casesToNA ,i ] <- NA
}
@

For each of the Hisei imputations 2, 3 and 4, we choose 12 random cases to insert a missing value. Will the function have to
surrender when the data structure is of this kind?\\

<<redefineValuesdef>>=
freqs <- jk2.table(dat = reading_writing2, ID = "idstud", wgt = "wgtSTUD",
      separate.missing.indikator = TRUE, JKZone = "JKZone", JKrep = "JKrep", 
	  group = list(federalState = "country", gender = "sex"), 
	  dependent = list(Hisei = paste("zehisei",1:5,sep="")))
freqs
@	  

Without bothering you unduly with the output, I only want to mention two details: First a new category has joined to the output,
which is labelled \code{"missing"}. Secondly, remember what we have done with our data disturbance. We replaced the "5" values
from the first and second imputation. Now, as group "5" no longer appears in both imputations, the distribution of the categories
now vary more widely between imputations. In the concept of multiple imputation this reflects the uncertainty due to missing 
data. Consequently, the standard errors especially for group "5" now have grown up.\\ 

\subsection{Quantiles}
\label{subsec:quantiles}

Estimation of quantiles for numerical variables is possible using the function \code{jk2.quantile}. All related analyses 
mentioned up to this point apply in the same way. Note that these analyses apply for numerical dependent variables. 
See the examples in the help file of \code{jk2.quantile()}.\\

\section{Generalized linear models}
\label{sec:ex2}

Considering multiple imputations and clustered structure in the estimation of generalized linear models is based on the same 
principes mentioned before. However, some additional comments due to specific characteristics of regression models have to be 
made. First we now have another type of variable---independent variables, which may occur as multiple imputed variables, too.
Second, we (optionally) have to specify the regression expression, what about we may are confused, as one variable occurs 
in different labels in multiple imputed data sets. Third, we will have to specify the kind of regression we propose to estimate, 
for example linear or logistic regression. We start with a simple example using the same data as before.\\

<<regression1def>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", group = list(country = "country"),
         dependent = list(reading = paste("reading_score", 1:3, sep = ""),
                          writing = paste("writing_score", 1:3, sep = "")),
         independent = list(gender = "sex", INCOME = c("income1", "income2")),
         complete.permutation = "no", glm.family = gaussian(link="identity") )
mod1[1]
@

As we might have expected, the outcome is a list of two data frames, according to the two dependent variables. Owing to space constraints,
only the first data frame of the result list is printed above. Please note that
each data frame contains results of more than one analysis! As we specified one group variable dividing the data into three distinct
groups, for which we instruct \code{jk2.glm()} to fit the regression model separately, we find results of the three models in each
data frame. More specifically, for each country, an intercept and two regression coefficients according to \code{gender} and
\code{INCOME} are estimated. If we would have used \code{gender} as an additional group variable instead of a predictor variable, the two
group variables would have divided the data into $3\times 2=6$ distinct groups (male vs. female in each of the three countries),
which would have result in six regression models for each of the two dependent variables.\\

Remember what was said about factors in the chapter about frequency tables: The gender variable now
has to be defined explicitly to be of class factor! Otherwise, albeit gender variable may be coded as 0/1, it would be treated to be a
continuous numeric variable. With only two levels---male and female---this may have no effect on the results, but consider a factor
variable with three levels, which may be coded 0, 1 and 2. We are interested in two coefficients which correspond
to the effect of level 1 vs. level 0 and the effect of level 2 vs. level 0. If we miss to define the variable to be of class factor,
only one coefficient is computed, and the variable is assumed to be continuous.
What we see additionally is that R implicitly defined the female group to be the reference---the
regression parameter was labelled \code{gendermale}.\\ 

Now we try something different. First we define gender to be our dependent variable. Secondly, we use country as a predictor. 
This is to test whether the proportions of gender vary between countries.\\ 

<<regression2def>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", dependent = list(gender = "sex"),
		 independent = list(federalState = "country") ,
         complete.permutation = "no", glm.family = binomial(link="logit") )
mod1
@

As we have no imputed variables, only one replication is run. No pooling has taken place. Although we have only defined one
independent variable, we obtain two regression coefficients for the two categories of the country variable. Again, R choosed 
its favorite reference group by itself. The effects are expressed in relation to \code{LandA}. To interpretate the effects, 
the coefficients may be transformed to odds ratios:\\

<<transformdef>>=
exp(mod1$gender[,"Estimate"])
@

In \code{LandB} the odds ratio to be male is 0.93 times the corresponding odds ratio in \code{LandA}. The following subsections
address three little questions one might ask oneself.

\subsection{How to change reference group at costumer's option}
\label{subsec:faq1}

As we saw in the preceding section, R choosed the reference group of factor variables by itself. Persuading R to meet our needs
is easier said than done. The essentially easiest way is a rather dummy method: recode the variable so that your favourite
reference group occurs at first when ordered alphabetically. We will demonstrate this procedure about the gender variable in our
fictitious data set. Remember the first example in section 3---R choosed the females to be the reference. Why? Simply because
female comes before male in the alphabet. Let's use the \code{recode()} function from the \code{car} package to define a new
variable \code{sexRecoded}:\\

<<recodeSexdef>>=
library(car)
reading_writing[,"sexRecoded"] <- car::recode(reading_writing[,"sex"], "'male' = '_male'")
@

The simple trick of adding a \code{"\_"} sign to the \code{"male"} label provokes R to sort \code{"\_male"}
before \code{"female"} alphabetically. Consequently, \code{"male"} is used as reference group when repeating the first example 
analysis of section 3.\\

<<regressionReplicationdef>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", group = list(country = "country"),
         dependent = list(reading = paste("reading_score", 1:3, sep = "")),
         independent = list(gender = "sexRecoded", INCOME = c("income1", "income2")),
         complete.permutation = "no", glm.family = gaussian(link="identity") )
mod1
@

\subsection{How to modify the regression statement}
\label{subsec:faq2}

When we call \code{glm()}, we are advised to specify the formula statement of the regression model. In \code{jk2.glm()}, however,
the regression statement is created automatically by the independent variables connected by a \code{"+"} symbol. As \code{jk2.glm()}
is advised to accomplish multiple imputations, the names of one and the same variable change between imputations. Consequently,
the regression statement is adapted between replications. However, regression models which go beyond simple addition of predictors,
have to be specified manually. The right side of the regression formula has to defined via the \code{reg.statement} argument 
as a string, whereas the
names of the independent variables occurring in the \code{independent} argument have to be used. We illustrate this procedure by
seizing on the preceding example. We have two independent variables, \code{gender} and \code{INCOME}. Without specifying any 
\code{reg.statement} argument, \code{jk2.glm()} implicitly uses \code{gender + INCOME}. Modelling an additional 
interaction of both variables needs to be defined explicitly:\\

<<regressionReplicationInteractiondef>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", JKZone = "JKZone",
         JKrep = "JKrep", group = list(country = "country"),
         dependent = list(reading = paste("reading_score", 1:3, sep = "")),
         independent = list(gender = "sexRecoded", INCOME = c("income1", "income2")),
         reg.statement = "gender * INCOME",
         complete.permutation = "no", glm.family = gaussian(link="identity") )
@

Please not that the regression statement---if specified---must contain all variables defined in the \code{independent} argument.
Note further that the names of the variables occurring in the \code{independent} argument are used instead of single imputation 
variable names. Don't try your luck with \code{reg.statement = "sexRecoded * income1"}! This logic implies that the list of
variables in the \code{independent} argument has to be named. Consequently, \code{independent = list("sexRecoded", c("income1", "income2"))}
will only work as long as no \code{reg.statement} argument is specified. Let's have a look at the output:\\

<<regressionReplicationInteractionOutputdef>>=
mod1
@

\subsection{Which of both determination coefficients should I pay attention?}
\label{subsec:faq3}

The output of each \code{jk2.glm()} analysis also contains the pooled determination coefficient, $R^2$, most frequently 
the conventional $R^2$ and Nagelkerke's $R^2$. However, in linear regression models, i.e. if the identity link is used, 
assuming normally distributed errors, the conventional $R^2$ should be used to interpret explained variance. In log-linear
regression models, i.e. if the binomial link function is used, Nagelkerke's $R^2$ should be used. The reason for reporting 
both coefficients is the programming disability of the package developer. 

\section{Nested imputations}
\label{sec:nested}

The next to last chapter of this little tutorial is reserved to the problem of nested imputation. The general concept is described in
\citet{Rubin2003}. At this point, only some specific aspects which are relevant in large scale assessments, are mentioned. Suppose you
want to estimate IRT proficiencies (often denotet $\theta$) in a specific domain. Applying an extensive marginal model which 
comprehends of items responses and background information as well, the posterior distribution of each examinees' $\theta$ is 
specified. Without any certain proficiency value of a specific examinee, plausible values 
are drawn from the posterior of each examinee. Conceptually, plausible values are multiple imputations of the inherently missing 
variable $\theta$ and may analyzed in standard statistic procedures according to the generalized linear model. To obtain valid
estimates and standard errors, the results have to be pooled according to \citet{Rubin1987}.\\

Suppose you have missing data in the background variables as well, which have to be imputed in the first step, which may result in 
$M=5$ data sets. For each data set a marginal IRT model is specified and $N=20$ plausible values are drawn. Overall 
$5\times 20=100$ plausible values in a dependency structure will result from the analysis. Formally, we now have nested imputed 
data. To pool the results, the formulas in \citet{Rubin1987} cannot
be applied, as the plausible values does not stem from a common ``nest''. The interdependence has to be taken into account. 
Whereas the conventional pooling formulas split the overall variance in the variance within imputation and the variance between 
imputation, where the latter one is used to estimate the uncertainty due to imputation, the formulas for nested imputation 
extend the old ones by splitting the variance between imputation in the within-nest variance and the variance between nests. 
See \citet{Rubin2003} for further details. These varied formulas are also implemented in \code{eatRep}.\\

If the data analysed with \code{eatRep} stem from a nested multiple imputation structure, this structure has to be specified. 
More specifically, \code{eatRep} has to know the number of nests and the number of imputations in each nest. Moreover, it has
to be specified which variable belongs to which imputation in which nest. The above procedure sounds more complicated than it 
hopefully is.\\

\subsection{Example: Compute descriptives from a nested imputation structure}
\label{subsec:nest1}

The \code{reading\_writing} data set ist not very proper for instructional purposes concerning nested imputations, but we will do 
our best. First consider the variables \code{"income1"} and \code{"income2"}. Suppose that these are only two imputations of
an \code{INCOME} variable which originally contains missing values. Therefore, we have $M=2$ nests. Now we assume that we have 
measured only one domain in our IRT analysis, which we call ``ability''. Suppose we draw three plausible values in each 
nest, therefore $N=3$. The corresponding variables are \code{"reading\_score1"}, \code{"reading\_score2"} and 
\code{"reading\_score3"} for the first nest, and \code{"writing\_score1"}, \code{"writing\_score2"} and 
\code{"writing\_score3"} for the second nest. (I am aware that the variable labelling is not disarmingly intuitive that way.)
Overall, we have $3\times 2=6$ plausible values in a dependency structure. We want to estimate descriptives for groups of
high vs. low income, and for each country separately. First we create an indicator of high vs. low income, where we set the 
threshold arbitrarily at 2000.\\

<<nestEx1def>>=
reading_writing[,"income_ind1"] <- factor(ifelse(reading_writing[,"income1"]>2000,"high","low"))
reading_writing[,"income_ind2"] <- factor(ifelse(reading_writing[,"income2"]>2000,"high","low"))
@

We than will have to specify two groups for which we want to compute descriptives. Remember that one group appears as a nested 
imputed variable:\\

<<nestEx2def>>=
means  <- jk2.mean(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD", 
          JKZone = "JKZone", JKrep = "JKrep", 
		  group = list(country = "country", INCOME = c("income_ind1","income_ind2")),
          dependent = list(ability = list(nest1 = paste("reading_score",1:3,sep=""),
                                          nest2 = paste("writing_score",1:3,sep="") )),
          complete.permutation = "no" )
@

The nested structure has to be specified in the \code{dependent} and \code{group} argument. The \code{dependent} argument now 
is no longer a list of character vectors but a list of lists. As \code{dependent} is a list of length 1, only one analysis is 
conducted. As the first and only element of \code{dependent} is a list instead of a character vector, a nested imputation 
structure is 
implied. As the length of the first element of \code{dependent} is 2, $M=2$ nests are assumed. Therefore, the function expects
to find group variables either without any imputations or with exactly $M=2$ imputations. Moreover, the plausible values of the 
first nest (labelled \code{"reading\_score1"}, \code{"reading\_score2"} and \code{"reading\_score3"}) are assumed to belong to
the first imputation of \code{INCOME}, labelled \code{"income1"}. Correspondingly, the plausible values of the 
second nest (labelled \code{"writing\_score1"}, \code{"writing\_score2"} and \code{"writing\_score3"}) are assumed to belong to
the second imputation of \code{INCOME}, labelled \code{"income2"}.\\

Conceptually, the \code{complete.permutation} argument should be set to \code{"nothing"} if a nested imputation structure is 
assumed. The output reveals rather high standard errors, as in out fictitious example the between-nest variance is rather high.\\

<<nestEx2Resultsdef>>=
means
@

\subsection{Example: Fit a linear regression model in a nested imputation structure}
\label{subsec:nest2}

The principles of considering the nested structure are quite the same as in the preceding example. For each country and the both
\code{income} groups we want to predict ``ability'' by \code{gender} and \code{INCOME}. Using \code{income} as group variable likewise
allows for investigating whether a potential effect of \code{INCOME} on ``ability'' differs in groups of high vs. low income.
Additionally, we model a possible interaction between \code{gender} and \code{INCOME}.\\

<<nestEx3def>>=
mod1  <- jk2.glm(dat = reading_writing, ID = "idstud", wgt = "wgtSTUD",
          JKZone = "JKZone", JKrep = "JKrep",
          group = list(country = "country", INCOME.group = c("income_ind1","income_ind2")),
          dependent = list(ability = list(nest1 = paste("reading_score",1:3,sep=""),
                                          nest2 = paste("writing_score",1:3,sep="") )),
          independent = list ( gender = "sex", INCOME = c("income1", "income2")),
          reg.statement = c("gender * INCOME"), glm.family = gaussian(link = "identity"),
          complete.permutation = "no" )
mod1
@

\section{Multicore processing}
\label{sec:multi}

The methods to handle imputed data and methods for cluster designs based on repeated analyses. Roughly spoken, (nearly) the same data
are analyzed many many times. To save time, the analyses may distributed to several logical processors, a procedure which is known as
``multicore processing''. In R, the \code{multicore} package provides functions for splitting the whole analysis in little single
components which are analyzed simultaneously.\\

By default, each of the four jackknife functions \code{jk2.mean}, \code{jk2.table}, \code{jk2.quantile} and \code{jk2.glm} has a
multicore counterpart, characterized by the suffix \code{.M}. Specifying the arguments works in the sammer manner as in the
singlecore functions, with two exceptions: First, the argument \code{group.differences.by} of the \code{jk2.mean} function
is not available in multicore processing. Second, several optional arguments by reference to multicore processing, may specified
in the \code{multicoreOptions} argument. If left out, R chooses the multicore options by itself.

\bibliography{eatrep}

\end{document}
